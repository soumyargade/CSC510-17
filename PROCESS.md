## Process
### Story Creation & Assignment
#### Iteration One
At the beginning of this iteration we divided each of our three use cases into stories & assigned story points as well as a developer to each of the tasks (as can be seen in the screenshot of our GitHub Project board below). For this iteration we plan to focus our efforts on fine tuning our web scraper's implementation as it is a foundational piece to the working of each of our use cases.

<img src="https://github.ncsu.edu/csc510-s2022/CSC510-17/blob/dev/img/iterationOne.png">

#### Iteration Two

## Practices
### Scrum Meeting Notes
#### Iteration One
| Date   | Progress & Blockers   |  Next Steps
| ------------- | ------------  |  ------------
| Sunday, March 20 | The team discussed work that will need to be completed for this sprint, created the necessary user stories, & added them to the project's Kanban board. | Soumya & Alex to research if there is a more viable alternative to ParseHub to use as the web scraper tool for scraping GitHub REST API documentation. Potential alternatives include the Cheerio library or Webscraper.io.
| Monday, March 21      | Soumya found Webscraper.io to be a better option than ParseHub due to ease of use with regards to its selector. However, there is a hefty price tag of $100/month in order to access the scraped data in a JSON format & use their API to programmatically retrieve data from scraping jobs. Dibya & Kim added user stories to the Kanban. |  <ul><li>**Kim**: Add story points + convert user stories in Kanban to GitHub issues. Assign each issue to a person.</li><li>**Dibya**: Merriam-Webster API integration.</li><li>**Alex**: Mattermost message handling & error handling.</li><li>**Soumya**: work on POC for Webscraper.io to determine if it can replace ParseHub.</li></ul>
| Tuesday, March 22      | <ul><li>**Kim**: Finished adding story points to each user story & converted them into issues.</li><li>**Dibya**: Created skeleton code for Merriam-Webster API.</li><li>**Alex**: Began error handling for invalid commands sent by the user for any of the use cases.</li><li>**Soumya**: Confirmed Webscraper.io is not a feasible option as parsing jobs aren't working when run from within the cloud, a feature that is necessary in order to access scraped data via API calls.</li></ul>             | <ul><li>**Kim**: </li><li>**Dibya**: Continue working on logic to pull correct HTTP verb based on the synonyms of the action the user chooses. </li><li>**Alex**: </li><li>**Soumya**: Work on improving ParseHub implementation from what was accomplished during the BOT milestone.</li></ul>
| Wednesday, March 23      | 2             | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Thursday, March 24      | 3             | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Friday, March 25      | &nbsp;        | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>

#### Iteration Two
| Date   | Progress & Blockers   |  Next Steps
| ------------- | ------------  |  ------------
| Monday, March 28      |           | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Tuesday, March 29      |              | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Wednesday, March 30      |              | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Thursday, March 31      |              | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
| Friday, April 1      |        | <ul><li>**Kim**: </li><li>**Dibya**: </li><li>**Alex**: </li><li>**Soumya**: </li></ul>
